{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AutoRec Abstract Realistic recommender systems are often required to adapt to ever-changing data and tasks or to explore different models systematically. To address the need, we present AutoRec , an open-source automated machine learning (AutoML) platform extended from the TensorFlow ecosystem and, to our knowledge, the first framework to leverage AutoML for model search and hyperparameter tuning in deep recommendation models. AutoRec also supports a highly flexible pipeline that accommodates both sparse and dense inputs, rating prediction and click-through rate (CTR) prediction tasks, and an array of recommendation models. Lastly, AutoRec provides a simple, user-friendly API. Experiments conducted on the benchmark datasets reveal AutoRec is reliable and can identify models which resemble the best model without prior knowledge. Demo Your browser does not support the video tag. Cite this work Wang, Ting-Hsiang, Xia Hu, Haifeng Jin, Qingquan Song, Xiaotian Han, and Zirui Liu. \"AutoRec: An Automated Recommender System.\" In Fourteenth ACM Conference on Recommender Systems. ACM, 2020. BibTex entry: @inproceedings { wang2020autorec , title = {AutoRec: An Automated Recommender System} , author = {Wang, Ting-Hsiang and Hu, Xia and Jin, Haifeng and Song, Qingquan and Han, Xiaotian and Liu, Zirui} , booktitle = {Fourteenth ACM Conference on Recommender Systems} , pages = {582--584} , year = {2020} }","title":"Home"},{"location":"#welcome-to-autorec","text":"","title":"Welcome to AutoRec"},{"location":"#abstract","text":"Realistic recommender systems are often required to adapt to ever-changing data and tasks or to explore different models systematically. To address the need, we present AutoRec , an open-source automated machine learning (AutoML) platform extended from the TensorFlow ecosystem and, to our knowledge, the first framework to leverage AutoML for model search and hyperparameter tuning in deep recommendation models. AutoRec also supports a highly flexible pipeline that accommodates both sparse and dense inputs, rating prediction and click-through rate (CTR) prediction tasks, and an array of recommendation models. Lastly, AutoRec provides a simple, user-friendly API. Experiments conducted on the benchmark datasets reveal AutoRec is reliable and can identify models which resemble the best model without prior knowledge.","title":"Abstract"},{"location":"#demo","text":"Your browser does not support the video tag.","title":"Demo"},{"location":"#cite-this-work","text":"Wang, Ting-Hsiang, Xia Hu, Haifeng Jin, Qingquan Song, Xiaotian Han, and Zirui Liu. \"AutoRec: An Automated Recommender System.\" In Fourteenth ACM Conference on Recommender Systems. ACM, 2020. BibTex entry: @inproceedings { wang2020autorec , title = {AutoRec: An Automated Recommender System} , author = {Wang, Ting-Hsiang and Hu, Xia and Jin, Haifeng and Song, Qingquan and Han, Xiaotian and Liu, Zirui} , booktitle = {Fourteenth ACM Conference on Recommender Systems} , pages = {582--584} , year = {2020} }","title":"Cite this work"},{"location":"about/","text":"This package is developed by DATA LAB at Texas A&M University. Core Team Ting-Hsiang Wang : Qingquan Song : Xiaotian Han : Zirui Liu : Haifeng Jin : Xia \"Ben\" Hu : Project lead and maintainer.","title":"About"},{"location":"about/#core-team","text":"Ting-Hsiang Wang : Qingquan Song : Xiaotian Han : Zirui Liu : Haifeng Jin : Xia \"Ben\" Hu : Project lead and maintainer.","title":"Core Team"},{"location":"auto_search/","text":"[source] Search autorecsys . auto_search . Search ( model = None , name = None , tuner = \"random\" , tuner_params = None , directory = \".\" , overwrite = True ) A search object to search on a Recommender HyperModel (CTRRecommender/RPRecommender) defined by inputs and outputs. Search combines a Recommender and a Tuner to tune the Recommender. The user can use search() to perform search, and use a similar way to a Keras model to adopt the best discovered model as it also has fit() / predict() / evaluate() methods. The user should input a Recommender HyperModel (CTRRecommender/RPRecommender) and a selected tuning method to initial the Search object and input the dataset when calling the search method to discover the best architecture. # Arguments model: A Recommender HyperModel (CTRRecommender/RPRecommender). name: String. The name of the project, which is used for saving and loading purposes. tuner: String. The name of the tuner. It should be one of 'greedy', 'bayesian' or 'random'. Default to be 'random'. tuner_params: Dict. The hyperparameters of the tuner. The commons ones are: 'max_trials': Int. Specify the number of search epochs. 'overwrite': Boolean. Whether we want to ovewrite an existing tuner or not. directory: String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the project in the current directory, i.e., ``directory/name``. overwrite: Boolean. Defaults to `True`. Whether we want to ovewrite an existing project with the name defined as ``directory/name`` or not. ---- <span style=\"float:right;\">[[source]](https://github.com/datamllab/AutoRecSys/autorecsys/auto_search.py#L67)</span> ### search ```python Search.search(x=None, y=None, x_val=None, y_val=None, objective=\"mse\", **fit_kwargs) Search the best deep recommendation model. Arguments x : numpy array. Training features. y : numpy array. Training targets. x_val : numpy array. Validation features. y_val : numpy array. Validation features. objective : String. Name of model metric to minimize or maximize, e.g. 'val_BinaryCrossentropy'. Defaults to 'mse'. **fit_kwargs : Any arguments supported by the fit method of a Keras model such as: batch_size , epochs , callbacks . [source] predict Search . predict ( x ) Use the best searched model to conduct prediction on the dataset x. Arguments x : numpy array / data frame / string path of a csv file. Features used to do the prediction. [source] evaluate Search . evaluate ( x , y_true ) Evaluate the best searched model. Arguments x : numpy array / data frame / string path of a csv file. Features used to do the prediction. y_true : numpy array / data frame / string path of a csv file. Ground-truth labels.","title":"Auto Search"},{"location":"auto_search/#search","text":"autorecsys . auto_search . Search ( model = None , name = None , tuner = \"random\" , tuner_params = None , directory = \".\" , overwrite = True ) A search object to search on a Recommender HyperModel (CTRRecommender/RPRecommender) defined by inputs and outputs. Search combines a Recommender and a Tuner to tune the Recommender. The user can use search() to perform search, and use a similar way to a Keras model to adopt the best discovered model as it also has fit() / predict() / evaluate() methods. The user should input a Recommender HyperModel (CTRRecommender/RPRecommender) and a selected tuning method to initial the Search object and input the dataset when calling the search method to discover the best architecture. # Arguments model: A Recommender HyperModel (CTRRecommender/RPRecommender). name: String. The name of the project, which is used for saving and loading purposes. tuner: String. The name of the tuner. It should be one of 'greedy', 'bayesian' or 'random'. Default to be 'random'. tuner_params: Dict. The hyperparameters of the tuner. The commons ones are: 'max_trials': Int. Specify the number of search epochs. 'overwrite': Boolean. Whether we want to ovewrite an existing tuner or not. directory: String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the project in the current directory, i.e., ``directory/name``. overwrite: Boolean. Defaults to `True`. Whether we want to ovewrite an existing project with the name defined as ``directory/name`` or not. ---- <span style=\"float:right;\">[[source]](https://github.com/datamllab/AutoRecSys/autorecsys/auto_search.py#L67)</span> ### search ```python Search.search(x=None, y=None, x_val=None, y_val=None, objective=\"mse\", **fit_kwargs) Search the best deep recommendation model. Arguments x : numpy array. Training features. y : numpy array. Training targets. x_val : numpy array. Validation features. y_val : numpy array. Validation features. objective : String. Name of model metric to minimize or maximize, e.g. 'val_BinaryCrossentropy'. Defaults to 'mse'. **fit_kwargs : Any arguments supported by the fit method of a Keras model such as: batch_size , epochs , callbacks . [source]","title":"Search"},{"location":"auto_search/#predict","text":"Search . predict ( x ) Use the best searched model to conduct prediction on the dataset x. Arguments x : numpy array / data frame / string path of a csv file. Features used to do the prediction. [source]","title":"predict"},{"location":"auto_search/#evaluate","text":"Search . evaluate ( x , y_true ) Evaluate the best searched model. Arguments x : numpy array / data frame / string path of a csv file. Features used to do the prediction. y_true : numpy array / data frame / string path of a csv file. Ground-truth labels.","title":"evaluate"},{"location":"benchmark/","text":"","title":"Benchmark"},{"location":"install/","text":"Requirements Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.4.0 : AutoRec is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup. Install AutoRec","title":"Installation"},{"location":"install/#requirements","text":"Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.4.0 : AutoRec is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup.","title":"Requirements"},{"location":"install/#install-autorec","text":"","title":"Install AutoRec"},{"location":"interactor/","text":"[source] RandomSelectInteraction autorecsys . pipeline . interactor . RandomSelectInteraction ( ** kwargs ) Module for output one vector select form the input vector list . Attributes: None [source] get_state RandomSelectInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state RandomSelectInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build RandomSelectInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] ConcatenateInteraction autorecsys . pipeline . interactor . ConcatenateInteraction ( ** kwargs ) Module for outputing one vector by concatenating the input vector list. Attributes: None [source] get_state ConcatenateInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state ConcatenateInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build ConcatenateInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] ElementwiseInteraction autorecsys . pipeline . interactor . ElementwiseInteraction ( elementwise_type = None , ** kwargs ) Module for element-wise operation. this block includes the element-wise sum ,average, innerporduct, max, and min. The default operation is average. Attributes: elementwise_type(\"str\"): Can be used to select the element-wise operation. the default value is None. If the value of this parameter is None, the block can select the operation for the sum ,average, innerporduct, max, and min, according to the search algorithm. [source] get_state ElementwiseInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state ElementwiseInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build ElementwiseInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] MLPInteraction autorecsys . pipeline . interactor . MLPInteraction ( units = None , num_layers = None , use_batchnorm = None , dropout_rate = None , ** kwargs ) Module for MLP operation. This block can seted as MLP with different layer, unit, and other setting . Attributes: units (int). The units of all layer in the MLP block. num_layers (int). The number of the layers in the MLP blck use_batchnorm (Boolean). Use batch normalization or not. dropout_rate(float). The value of drop out in the last layer of MLP. [source] get_state MLPInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state MLPInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build MLPInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] HyperInteraction autorecsys . pipeline . interactor . HyperInteraction ( meta_interator_num = None , interactor_type = None , ** kwargs ) Module for selecting different block. This block includes can select different blocks in the interactor. Attributes: meta_interator_num (str). The total number of the meta interoctor block. interactor_type (str). The type of interactor used in this block. [source] get_state HyperInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state HyperInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build HyperInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] FMInteraction autorecsys . pipeline . interactor . FMInteraction ( embedding_dim = None , ** kwargs ) CTR module for factorization machine operation. Reference: https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf This block applies factorization machine operation on a list of input 3D tensors of size (batch_size, field_size, embedding_size). It will align the dimension of tensors to 3D if they're 1D or 2D originally, and will align/transfrom the last embedding dimension based on a tunable hyperaparmeter. Attributes: embedding_dim (int). The transformed embedding dimension of each field, before conducting the factorization machine operation. [source] get_state FMInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state FMInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build FMInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] CrossNetInteraction autorecsys . pipeline . interactor . CrossNetInteraction ( layer_num = None , ** kwargs ) CTR module for crossnet layer in deep & cross network. Reference: https://arxiv.org/pdf/1708.05123.pdf This block applies cross interaction operation on a 2D tensors of size (batch_size, embedding_size). We assume the input could be a list of tensors of 2D or 3D, and the block will flatten them as as list of 2D tensors, and ten concatenate them as a single 2D tensor. The cross interaction follows the reference and the number of layers of the cross interaction is tunable. Attributes: layer_num (int). The number of layers of the cross interaction. [source] get_state CrossNetInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state CrossNetInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build CrossNetInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] SelfAttentionInteraction autorecsys . pipeline . interactor . SelfAttentionInteraction ( embedding_dim = None , att_embedding_dim = None , head_num = None , residual = None , ** kwargs ) CTR module for the multi-head self-attention layer in the autoint paper. Reference: https://arxiv.org/pdf/1708.05123.pdf This block applies multi-head self-attention on a 3D tensor of size (batch_size, field_size, embedding_size). We assume the input could be a list of tensors of 1D, 2D or 3D, and the block will align the dimension of tensors to 3D if they're 1D or 2D originally, and it will also align the last embedding dimension based on a tunable hyperaparmeter. Attributes: embedding_dim (int). Embedding dimension for aligning embedding dimension of the input tensors. att_embedding_dim (int). Output embedding dimension after the mulit-head self-attention. head_num (int). Number of attention heads. residual (boolean). Whether to apply residual connection after self-attention or not. [source] get_state SelfAttentionInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state SelfAttentionInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build SelfAttentionInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"Interactor"},{"location":"interactor/#randomselectinteraction","text":"autorecsys . pipeline . interactor . RandomSelectInteraction ( ** kwargs ) Module for output one vector select form the input vector list . Attributes: None [source]","title":"RandomSelectInteraction"},{"location":"interactor/#get_state","text":"RandomSelectInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state","text":"RandomSelectInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build","text":"RandomSelectInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#concatenateinteraction","text":"autorecsys . pipeline . interactor . ConcatenateInteraction ( ** kwargs ) Module for outputing one vector by concatenating the input vector list. Attributes: None [source]","title":"ConcatenateInteraction"},{"location":"interactor/#get_state_1","text":"ConcatenateInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_1","text":"ConcatenateInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_1","text":"ConcatenateInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#elementwiseinteraction","text":"autorecsys . pipeline . interactor . ElementwiseInteraction ( elementwise_type = None , ** kwargs ) Module for element-wise operation. this block includes the element-wise sum ,average, innerporduct, max, and min. The default operation is average. Attributes: elementwise_type(\"str\"): Can be used to select the element-wise operation. the default value is None. If the value of this parameter is None, the block can select the operation for the sum ,average, innerporduct, max, and min, according to the search algorithm. [source]","title":"ElementwiseInteraction"},{"location":"interactor/#get_state_2","text":"ElementwiseInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_2","text":"ElementwiseInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_2","text":"ElementwiseInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#mlpinteraction","text":"autorecsys . pipeline . interactor . MLPInteraction ( units = None , num_layers = None , use_batchnorm = None , dropout_rate = None , ** kwargs ) Module for MLP operation. This block can seted as MLP with different layer, unit, and other setting . Attributes: units (int). The units of all layer in the MLP block. num_layers (int). The number of the layers in the MLP blck use_batchnorm (Boolean). Use batch normalization or not. dropout_rate(float). The value of drop out in the last layer of MLP. [source]","title":"MLPInteraction"},{"location":"interactor/#get_state_3","text":"MLPInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_3","text":"MLPInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_3","text":"MLPInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#hyperinteraction","text":"autorecsys . pipeline . interactor . HyperInteraction ( meta_interator_num = None , interactor_type = None , ** kwargs ) Module for selecting different block. This block includes can select different blocks in the interactor. Attributes: meta_interator_num (str). The total number of the meta interoctor block. interactor_type (str). The type of interactor used in this block. [source]","title":"HyperInteraction"},{"location":"interactor/#get_state_4","text":"HyperInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_4","text":"HyperInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_4","text":"HyperInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#fminteraction","text":"autorecsys . pipeline . interactor . FMInteraction ( embedding_dim = None , ** kwargs ) CTR module for factorization machine operation. Reference: https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf This block applies factorization machine operation on a list of input 3D tensors of size (batch_size, field_size, embedding_size). It will align the dimension of tensors to 3D if they're 1D or 2D originally, and will align/transfrom the last embedding dimension based on a tunable hyperaparmeter. Attributes: embedding_dim (int). The transformed embedding dimension of each field, before conducting the factorization machine operation. [source]","title":"FMInteraction"},{"location":"interactor/#get_state_5","text":"FMInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_5","text":"FMInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_5","text":"FMInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#crossnetinteraction","text":"autorecsys . pipeline . interactor . CrossNetInteraction ( layer_num = None , ** kwargs ) CTR module for crossnet layer in deep & cross network. Reference: https://arxiv.org/pdf/1708.05123.pdf This block applies cross interaction operation on a 2D tensors of size (batch_size, embedding_size). We assume the input could be a list of tensors of 2D or 3D, and the block will flatten them as as list of 2D tensors, and ten concatenate them as a single 2D tensor. The cross interaction follows the reference and the number of layers of the cross interaction is tunable. Attributes: layer_num (int). The number of layers of the cross interaction. [source]","title":"CrossNetInteraction"},{"location":"interactor/#get_state_6","text":"CrossNetInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_6","text":"CrossNetInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_6","text":"CrossNetInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#selfattentioninteraction","text":"autorecsys . pipeline . interactor . SelfAttentionInteraction ( embedding_dim = None , att_embedding_dim = None , head_num = None , residual = None , ** kwargs ) CTR module for the multi-head self-attention layer in the autoint paper. Reference: https://arxiv.org/pdf/1708.05123.pdf This block applies multi-head self-attention on a 3D tensor of size (batch_size, field_size, embedding_size). We assume the input could be a list of tensors of 1D, 2D or 3D, and the block will align the dimension of tensors to 3D if they're 1D or 2D originally, and it will also align the last embedding dimension based on a tunable hyperaparmeter. Attributes: embedding_dim (int). Embedding dimension for aligning embedding dimension of the input tensors. att_embedding_dim (int). Output embedding dimension after the mulit-head self-attention. head_num (int). Number of attention heads. residual (boolean). Whether to apply residual connection after self-attention or not. [source]","title":"SelfAttentionInteraction"},{"location":"interactor/#get_state_7","text":"SelfAttentionInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_7","text":"SelfAttentionInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_7","text":"SelfAttentionInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"build"},{"location":"mapper/","text":"[source] LatentFactorMapper autorecsys . pipeline . mapper . LatentFactorMapper ( feat_column_id = None , id_num = None , embedding_dim = None , ** kwargs ) Module for mapping the user/item id to the laten factor. Attributes: feat_column_id (int): the id of the used feateure. id_num (int): The total number of the user/item id. embedding_dim (int): The embedding size of the latent factor. [source] get_state LatentFactorMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state LatentFactorMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build LatentFactorMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] DenseFeatureMapper autorecsys . pipeline . mapper . DenseFeatureMapper ( num_of_fields = None , embedding_dim = None , ** kwargs ) Mapper for dense feature that can map dense feature to embedding. Attributes: num_of_fields (int): the number of feature fields. embedding_dim (int): The embedding size of the latent factor. [source] get_state DenseFeatureMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state DenseFeatureMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build DenseFeatureMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] SparseFeatureMapper autorecsys . pipeline . mapper . SparseFeatureMapper ( num_of_fields = None , hash_size = None , embedding_dim = None , ** kwargs ) Mapper for sparse feature that can map categorical features to embedding. Attributes: num_of_fields (int): the number of feature fields. hash_size (int): size for every feature. embedding_dim (int): The embedding size of the latent factor. [source] get_state SparseFeatureMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state SparseFeatureMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build SparseFeatureMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"Mapper"},{"location":"mapper/#latentfactormapper","text":"autorecsys . pipeline . mapper . LatentFactorMapper ( feat_column_id = None , id_num = None , embedding_dim = None , ** kwargs ) Module for mapping the user/item id to the laten factor. Attributes: feat_column_id (int): the id of the used feateure. id_num (int): The total number of the user/item id. embedding_dim (int): The embedding size of the latent factor. [source]","title":"LatentFactorMapper"},{"location":"mapper/#get_state","text":"LatentFactorMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"mapper/#set_state","text":"LatentFactorMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"mapper/#build","text":"LatentFactorMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"mapper/#densefeaturemapper","text":"autorecsys . pipeline . mapper . DenseFeatureMapper ( num_of_fields = None , embedding_dim = None , ** kwargs ) Mapper for dense feature that can map dense feature to embedding. Attributes: num_of_fields (int): the number of feature fields. embedding_dim (int): The embedding size of the latent factor. [source]","title":"DenseFeatureMapper"},{"location":"mapper/#get_state_1","text":"DenseFeatureMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"mapper/#set_state_1","text":"DenseFeatureMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"mapper/#build_1","text":"DenseFeatureMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"mapper/#sparsefeaturemapper","text":"autorecsys . pipeline . mapper . SparseFeatureMapper ( num_of_fields = None , hash_size = None , embedding_dim = None , ** kwargs ) Mapper for sparse feature that can map categorical features to embedding. Attributes: num_of_fields (int): the number of feature fields. hash_size (int): size for every feature. embedding_dim (int): The embedding size of the latent factor. [source]","title":"SparseFeatureMapper"},{"location":"mapper/#get_state_2","text":"SparseFeatureMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"mapper/#set_state_2","text":"SparseFeatureMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"mapper/#build_2","text":"SparseFeatureMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"build"},{"location":"node/","text":"[source] Input autorecsys . pipeline . node . Input ( shape = None ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. [source] fit_transform Input . fit_transform ( x ) [source] transform Input . transform ( x ) Transform x into a compatible type (tf.data.Dataset). [source] StructuredDataInput autorecsys . pipeline . node . StructuredDataInput ( column_names = None , column_types = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. [source] get_state StructuredDataInput . get_state () [source] set_state StructuredDataInput . set_state ( state ) [source] update StructuredDataInput . update ( x ) [source] infer_column_types StructuredDataInput . infer_column_types ()","title":"Node"},{"location":"node/#input","text":"autorecsys . pipeline . node . Input ( shape = None ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. [source]","title":"Input"},{"location":"node/#fit_transform","text":"Input . fit_transform ( x ) [source]","title":"fit_transform"},{"location":"node/#transform","text":"Input . transform ( x ) Transform x into a compatible type (tf.data.Dataset). [source]","title":"transform"},{"location":"node/#structureddatainput","text":"autorecsys . pipeline . node . StructuredDataInput ( column_names = None , column_types = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. [source]","title":"StructuredDataInput"},{"location":"node/#get_state","text":"StructuredDataInput . get_state () [source]","title":"get_state"},{"location":"node/#set_state","text":"StructuredDataInput . set_state ( state ) [source]","title":"set_state"},{"location":"node/#update","text":"StructuredDataInput . update ( x ) [source]","title":"update"},{"location":"node/#infer_column_types","text":"StructuredDataInput . infer_column_types ()","title":"infer_column_types"},{"location":"optimizer/","text":"[source] RatingPredictionOptimizer autorecsys . pipeline . optimizer . RatingPredictionOptimizer ( ** kwargs ) Module for rating prediction task. Attributes: None. [source] build RatingPredictionOptimizer . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] PointWiseOptimizer autorecsys . pipeline . optimizer . PointWiseOptimizer ( name = None , ** kwargs ) Module for click through rate prediction. Attributes: None. [source] build PointWiseOptimizer . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"Optimizer"},{"location":"optimizer/#ratingpredictionoptimizer","text":"autorecsys . pipeline . optimizer . RatingPredictionOptimizer ( ** kwargs ) Module for rating prediction task. Attributes: None. [source]","title":"RatingPredictionOptimizer"},{"location":"optimizer/#build","text":"RatingPredictionOptimizer . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"optimizer/#pointwiseoptimizer","text":"autorecsys . pipeline . optimizer . PointWiseOptimizer ( name = None , ** kwargs ) Module for click through rate prediction. Attributes: None. [source]","title":"PointWiseOptimizer"},{"location":"optimizer/#build_1","text":"PointWiseOptimizer . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"build"},{"location":"preprocessor/","text":"[source] BasePreprocessor autorecsys . pipeline . preprocessor . BasePreprocessor ( non_csv_path = None , csv_path = None , header = None , columns = None , delimiter = None , filler = None , dtype_dict = None , ignored_columns = None , target_column = None , numerical_columns = None , categorical_columns = None , categorical_filter = None , fit_dictionary_path = None , transform_path = None , test_percentage = None , validate_percentage = None , train_path = None , validate_path = None , test_path = None , ) Preprocess data into Pandas DataFrame format. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. Attributes non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. data_df (DataFrame) : Data loaded in Pandas DataFrame format and contains only relevant columns. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dict (dict) : Map string categorical column names to dictionary which maps categories to indices. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] format_dataset BasePreprocessor . format_dataset () (Optional) Convert dataset into CSV format. Note User should implement this function to convert non-CSV dataset into CSV format. [source] load_dataset BasePreprocessor . load_dataset () Load CSV data as a Pandas DataFrame object. [source] transform_categorical BasePreprocessor . transform_categorical () Transform categorical data. Note Produce fit dictionary for categorical data and transform categorical data using fit dictionary. [source] transform_numerical BasePreprocessor . transform_numerical () Transform numerical data using supported data transformation functions. [source] get_hash_size BasePreprocessor . get_hash_size () Get the hash sizes of categorical columns. Returns List of integer numbers of categories in each categorical data columns. [source] get_x BasePreprocessor . get_x () Get the training data columns. Returns DataFrame training data columns. [source] get_x_numerical BasePreprocessor . get_x_numerical ( x ) Get the numerical columns from the input data columns. Arguments x (DataFrame) : The input data columns. Returns ndarray numerical columns in the input data columns. [source] get_x_categorical BasePreprocessor . get_x_categorical ( x ) Get the categorical columns from the input data columns. Arguments x (DataFrame) : The input data columns. Returns ndarray categorical columns in the input data columns. [source] get_y BasePreprocessor . get_y () Get the output column. Returns ndarray output column. [source] get_numerical_count BasePreprocessor . get_numerical_count () Get the number of numerical columns. Returns Integer number of numerical columns. [source] get_categorical_count BasePreprocessor . get_categorical_count () Get the number of categorical columns. Returns Integer number of categorical columns. [source] split_data BasePreprocessor . split_data ( x , y , test_percentage ) Split data into the train, validation, and test sets. Arguments x (ndarray) : (M, N) matrix associated with the numerical and categorical data, where M is the number of rows and N is the number of numerical and categorical columns. y (ndarray) : (M, 1) matrix associated with the label data, where M is the number of rows. test_percentage (float) : Percentage of test set. Returns 4-tuple of ndarray training input data, testing input data, training output data, and testing output data. [source] preprocess BasePreprocessor . preprocess () Apply all preprocess steps. Note User is responsible for calling the needed data preprocessing functions from here. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source] AvazuPreprocessor autorecsys . pipeline . preprocessor . AvazuPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/avazu/train-10k\" , header = 0 , columns = None , delimiter = \",\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"click\" , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Avazu dataset for click-through rate (CTR) prediction. Note: To obtain the Avazu dataset, visit: https://www.kaggle.com/c/avazu-ctr-prediction The Avazu dataset has 24 data columns: [0] is ignored, [1] is label, and [2-23] are categorical. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] preprocess AvazuPreprocessor . preprocess () Apply all preprocessing steps to the Avazu dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source] CriteoPreprocessor autorecsys . pipeline . preprocessor . CriteoPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/criteo/train-10k.txt\" , header = None , columns = None , delimiter = \" \\t \" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = 0 , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Criteo dataset for click-through rate (CTR) prediction. Note To obtain the Criteo dataset, visit: https://www.kaggle.com/c/criteo-display-ad-challenge/ The Criteo dataset has 40 data columns: [0] is label, [1-13] are numerical, and [14-39] are categorical. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] preprocess CriteoPreprocessor . preprocess () Apply all preprocessing steps to the Criteo dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source] NetflixPrizePreprocessor autorecsys . pipeline . preprocessor . NetflixPrizePreprocessor ( non_csv_path = \"./example_datasets/netflix/combined_data_1-10k.txt\" , csv_path = \"./example_datasets/netflix/combined_data_1-10k.csv\" , header = None , columns = None , delimiter = \",\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"Rating\" , numerical_columns = None , categorical_columns = None , categorical_filter = 0 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Netflix dataset for rating prediction. Note To obtain the Netflix dataset, visit: https://www.kaggle.com/netflix-inc/netflix-prize-data The Netflix dataset has 4 data columns: MovieID, CustomerID, Rating, and Date. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] format_dataset NetflixPrizePreprocessor . format_dataset () Convert the Netflix Prize dataset into CSV format and save it as a new file. Note: This is an example showing the function which converts dataset into the CSV format as provided by user. [source] preprocess NetflixPrizePreprocessor . preprocess () Apply all preprocessing steps to the Netflix Prize dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source] MovielensPreprocessor autorecsys . pipeline . preprocessor . MovielensPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/movielens/ratings-10k.dat\" , header = None , columns = None , delimiter = \"::\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"Rating\" , numerical_columns = None , categorical_columns = None , categorical_filter = 0 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Movielens 1M dataset for rating prediction. Note To obtain the Movielens 1M dataset, visit: https://grouplens.org/datasets/movielens/ The Movielens 1M dataset has 4 data columns: UserID, MovieID, Rating, and Timestamp. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] preprocess MovielensPreprocessor . preprocess () Apply all preprocessing steps to the Movielens 1M dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data.","title":"Preprocessor"},{"location":"preprocessor/#basepreprocessor","text":"autorecsys . pipeline . preprocessor . BasePreprocessor ( non_csv_path = None , csv_path = None , header = None , columns = None , delimiter = None , filler = None , dtype_dict = None , ignored_columns = None , target_column = None , numerical_columns = None , categorical_columns = None , categorical_filter = None , fit_dictionary_path = None , transform_path = None , test_percentage = None , validate_percentage = None , train_path = None , validate_path = None , test_path = None , ) Preprocess data into Pandas DataFrame format. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. Attributes non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. data_df (DataFrame) : Data loaded in Pandas DataFrame format and contains only relevant columns. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dict (dict) : Map string categorical column names to dictionary which maps categories to indices. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"BasePreprocessor"},{"location":"preprocessor/#format_dataset","text":"BasePreprocessor . format_dataset () (Optional) Convert dataset into CSV format. Note User should implement this function to convert non-CSV dataset into CSV format. [source]","title":"format_dataset"},{"location":"preprocessor/#load_dataset","text":"BasePreprocessor . load_dataset () Load CSV data as a Pandas DataFrame object. [source]","title":"load_dataset"},{"location":"preprocessor/#transform_categorical","text":"BasePreprocessor . transform_categorical () Transform categorical data. Note Produce fit dictionary for categorical data and transform categorical data using fit dictionary. [source]","title":"transform_categorical"},{"location":"preprocessor/#transform_numerical","text":"BasePreprocessor . transform_numerical () Transform numerical data using supported data transformation functions. [source]","title":"transform_numerical"},{"location":"preprocessor/#get_hash_size","text":"BasePreprocessor . get_hash_size () Get the hash sizes of categorical columns. Returns List of integer numbers of categories in each categorical data columns. [source]","title":"get_hash_size"},{"location":"preprocessor/#get_x","text":"BasePreprocessor . get_x () Get the training data columns. Returns DataFrame training data columns. [source]","title":"get_x"},{"location":"preprocessor/#get_x_numerical","text":"BasePreprocessor . get_x_numerical ( x ) Get the numerical columns from the input data columns. Arguments x (DataFrame) : The input data columns. Returns ndarray numerical columns in the input data columns. [source]","title":"get_x_numerical"},{"location":"preprocessor/#get_x_categorical","text":"BasePreprocessor . get_x_categorical ( x ) Get the categorical columns from the input data columns. Arguments x (DataFrame) : The input data columns. Returns ndarray categorical columns in the input data columns. [source]","title":"get_x_categorical"},{"location":"preprocessor/#get_y","text":"BasePreprocessor . get_y () Get the output column. Returns ndarray output column. [source]","title":"get_y"},{"location":"preprocessor/#get_numerical_count","text":"BasePreprocessor . get_numerical_count () Get the number of numerical columns. Returns Integer number of numerical columns. [source]","title":"get_numerical_count"},{"location":"preprocessor/#get_categorical_count","text":"BasePreprocessor . get_categorical_count () Get the number of categorical columns. Returns Integer number of categorical columns. [source]","title":"get_categorical_count"},{"location":"preprocessor/#split_data","text":"BasePreprocessor . split_data ( x , y , test_percentage ) Split data into the train, validation, and test sets. Arguments x (ndarray) : (M, N) matrix associated with the numerical and categorical data, where M is the number of rows and N is the number of numerical and categorical columns. y (ndarray) : (M, 1) matrix associated with the label data, where M is the number of rows. test_percentage (float) : Percentage of test set. Returns 4-tuple of ndarray training input data, testing input data, training output data, and testing output data. [source]","title":"split_data"},{"location":"preprocessor/#preprocess","text":"BasePreprocessor . preprocess () Apply all preprocess steps. Note User is responsible for calling the needed data preprocessing functions from here. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source]","title":"preprocess"},{"location":"preprocessor/#avazupreprocessor","text":"autorecsys . pipeline . preprocessor . AvazuPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/avazu/train-10k\" , header = 0 , columns = None , delimiter = \",\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"click\" , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Avazu dataset for click-through rate (CTR) prediction. Note: To obtain the Avazu dataset, visit: https://www.kaggle.com/c/avazu-ctr-prediction The Avazu dataset has 24 data columns: [0] is ignored, [1] is label, and [2-23] are categorical. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"AvazuPreprocessor"},{"location":"preprocessor/#preprocess_1","text":"AvazuPreprocessor . preprocess () Apply all preprocessing steps to the Avazu dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source]","title":"preprocess"},{"location":"preprocessor/#criteopreprocessor","text":"autorecsys . pipeline . preprocessor . CriteoPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/criteo/train-10k.txt\" , header = None , columns = None , delimiter = \" \\t \" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = 0 , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Criteo dataset for click-through rate (CTR) prediction. Note To obtain the Criteo dataset, visit: https://www.kaggle.com/c/criteo-display-ad-challenge/ The Criteo dataset has 40 data columns: [0] is label, [1-13] are numerical, and [14-39] are categorical. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"CriteoPreprocessor"},{"location":"preprocessor/#preprocess_2","text":"CriteoPreprocessor . preprocess () Apply all preprocessing steps to the Criteo dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source]","title":"preprocess"},{"location":"preprocessor/#netflixprizepreprocessor","text":"autorecsys . pipeline . preprocessor . NetflixPrizePreprocessor ( non_csv_path = \"./example_datasets/netflix/combined_data_1-10k.txt\" , csv_path = \"./example_datasets/netflix/combined_data_1-10k.csv\" , header = None , columns = None , delimiter = \",\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"Rating\" , numerical_columns = None , categorical_columns = None , categorical_filter = 0 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Netflix dataset for rating prediction. Note To obtain the Netflix dataset, visit: https://www.kaggle.com/netflix-inc/netflix-prize-data The Netflix dataset has 4 data columns: MovieID, CustomerID, Rating, and Date. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"NetflixPrizePreprocessor"},{"location":"preprocessor/#format_dataset_1","text":"NetflixPrizePreprocessor . format_dataset () Convert the Netflix Prize dataset into CSV format and save it as a new file. Note: This is an example showing the function which converts dataset into the CSV format as provided by user. [source]","title":"format_dataset"},{"location":"preprocessor/#preprocess_3","text":"NetflixPrizePreprocessor . preprocess () Apply all preprocessing steps to the Netflix Prize dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source]","title":"preprocess"},{"location":"preprocessor/#movielenspreprocessor","text":"autorecsys . pipeline . preprocessor . MovielensPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/movielens/ratings-10k.dat\" , header = None , columns = None , delimiter = \"::\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"Rating\" , numerical_columns = None , categorical_columns = None , categorical_filter = 0 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Movielens 1M dataset for rating prediction. Note To obtain the Movielens 1M dataset, visit: https://grouplens.org/datasets/movielens/ The Movielens 1M dataset has 4 data columns: UserID, MovieID, Rating, and Timestamp. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"MovielensPreprocessor"},{"location":"preprocessor/#preprocess_4","text":"MovielensPreprocessor . preprocess () Apply all preprocessing steps to the Movielens 1M dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data.","title":"preprocess"},{"location":"recommender/","text":"[source] RPRecommender autorecsys . recommender . RPRecommender ( ** kwargs ) A Rating-Prediction HyperModel based on connected Blocks and HyperBlocks. Arguments inputs : A list of input node(s) for the HyperGraph. outputs : A list of output node(s) for the HyperGraph. [source] CTRRecommender autorecsys . recommender . CTRRecommender ( ** kwargs ) A CTR-Prediction HyperModel based on connected Blocks and HyperBlocks. Arguments inputs : A list of input node(s) for the HyperGraph. outputs : A list of output node(s) for the HyperGraph.","title":"Recommender"},{"location":"recommender/#rprecommender","text":"autorecsys . recommender . RPRecommender ( ** kwargs ) A Rating-Prediction HyperModel based on connected Blocks and HyperBlocks. Arguments inputs : A list of input node(s) for the HyperGraph. outputs : A list of output node(s) for the HyperGraph. [source]","title":"RPRecommender"},{"location":"recommender/#ctrrecommender","text":"autorecsys . recommender . CTRRecommender ( ** kwargs ) A CTR-Prediction HyperModel based on connected Blocks and HyperBlocks. Arguments inputs : A list of input node(s) for the HyperGraph. outputs : A list of output node(s) for the HyperGraph.","title":"CTRRecommender"},{"location":"examples/ctr_autoint/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , SelfAttentionInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # Step 1: Preprocess data criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () # Step 2: Build the recommender, which provides search space dense_input_node = Input ( shape = [ numerical_count ]) sparse_input_node = Input ( shape = [ categorical_count ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) attention_output = SelfAttentionInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ attention_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # Step 3: Build the searcher, which provides search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) # Step 4: Use the searcher to search the recommender searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) # Step 5: Evaluate the searched model logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = val_y )))","title":"Ctr autoint"},{"location":"examples/ctr_autorec/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , HyperInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # Step 1: Preprocess data criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # Step 2: Build model dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) sparse_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_emb ]) dense_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ dense_feat_emb ]) hyper_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_bottom_output , dense_feat_bottom_output ]) output = PointWiseOptimizer ()( hyper_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # Step 3: Build Searcher searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) # Step 4: Search model & HP searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr autorec"},{"location":"examples/ctr_benchmark/","text":"import argparse import time import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"1\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , DenseFeatureMapper , SparseFeatureMapper , \\ ElementwiseInteraction , FMInteraction , MLPInteraction , ConcatenateInteraction , \\ CrossNetInteraction , SelfAttentionInteraction , HyperInteraction , \\ PointWiseOptimizer from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor from autorecsys.recommender import CTRRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) def build_dlrm ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] output = MLPInteraction ( num_layers = 2 )( emb_list ) else : sparse_feat_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] dense_feat_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( sparse_feat_mlp_output + dense_feat_mlp_output ) return output def build_deepfm ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] fm_output = [ FMInteraction ()( emb_list )] bottom_mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) else : fm_output = [ FMInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] bottom_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) return output def build_crossnet ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] fm_output = [ CrossNetInteraction ()( emb_list )] bottom_mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) else : fm_output = [ CrossNetInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] bottom_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) return output def build_autoint ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] fm_output = [ SelfAttentionInteraction ()( emb_list )] bottom_mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) else : fm_output = [ SelfAttentionInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] bottom_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) return output def build_neumf ( emb_dict ): emb_list = [ emb for _ , emb in emb_dict . items ()] innerproduct_output = [ ElementwiseInteraction ( elementwise_type = \"innerporduct\" )( emb_list )] mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = innerproduct_output + mlp_output return output def build_autorec ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] output = HyperInteraction ()( emb_list ) else : sparse_feat_bottom_output = [ HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_emb ])] if 'sparse' in emb_dict else [] dense_feat_bottom_output = [ HyperInteraction ( meta_interator_num = 2 )([ dense_feat_emb ])] if 'dense' in emb_dict else [] top_mlp_output = HyperInteraction ( meta_interator_num = 2 )( sparse_feat_bottom_output + dense_feat_bottom_output ) output = HyperInteraction ( meta_interator_num = 2 )([ top_mlp_output ]) return output if __name__ == '__main__' : # parse args parser = argparse . ArgumentParser () parser . add_argument ( '-model' , type = str , help = 'input a model name' , default = 'dlrm' ) parser . add_argument ( '-data' , type = str , help = 'dataset name' , default = \"avazu\" ) parser . add_argument ( '-data_path' , type = str , help = 'dataset path' , default = './datasets/avazu/avazu_500K.txt' ) parser . add_argument ( '-sep' , type = str , help = 'dataset sep' ) parser . add_argument ( '-search' , type = str , help = 'input a search method name' , default = 'random' ) parser . add_argument ( '-batch_size' , type = int , help = 'batch size' , default = 256 ) parser . add_argument ( '-trials' , type = int , help = 'try number' , default = 10 ) parser . add_argument ( '-gpu_index' , type = int , help = 'the index of gpu to use' , default = 0 ) args = parser . parse_args () print ( \"args:\" , args ) os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = str ( args . gpu_index ) if args . sep == None : args . sep = '::' # Load and preprocess dataset if args . data == \"avazu\" : avazu = AvazuPreprocessor ( csv_path = args . data_path , validate_percentage = 0.1 , test_percentage = 0.1 ) train_X , train_y , val_X , val_y , test_X , test_y = avazu . preprocess () # dense_input_node = None sparse_input_node = Input ( shape = [ avazu . get_categorical_count ()]) input = [ sparse_input_node ] # dense_feat_emb = None sparse_feat_emb = SparseFeatureMapper ( num_of_fields = avazu . get_categorical_count (), hash_size = avazu . get_hash_size (), embedding_dim = 64 )( sparse_input_node ) emb_dict = { 'sparse' : sparse_feat_emb } if args . data == \"criteo\" : criteo = CriteoPreprocessor ( csv_path = args . data_path , validate_percentage = 0.1 , test_percentage = 0.1 ) train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_categorical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) input = [ dense_input_node , sparse_input_node ] dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 64 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 64 )( sparse_input_node ) emb_dict = { 'dense' : dense_feat_emb , 'sparse' : sparse_feat_emb } # select model if args . model == 'dlrm' : output = build_dlrm ( emb_dict ) if args . model == 'deepfm' : output = build_deepfm ( emb_dict ) if args . model == 'crossnet' : output = build_neumf ( emb_dict ) if args . model == 'autoint' : output = build_autorec ( emb_dict ) # if args.model == 'neumf': # output = build_autorec(emb_dict) if args . model == 'autorec' : output = build_autorec ( emb_dict ) output = PointWiseOptimizer ()( output ) model = CTRRecommender ( inputs = input , outputs = output ) # search and predict. searcher = Search ( model = model , tuner = args . search , tuner_params = { 'max_trials' : args . trials , 'overwrite' : True } ) start_time = time . time () searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = args . batch_size , epochs = 1 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) end_time = time . time () print ( \"running time:\" , end_time - start_time ) print ( \"args\" , args ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X [: 10 ]))) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X , y_true = test_y )))","title":"Ctr benchmark"},{"location":"examples/ctr_crossnet/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , CrossNetInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) crossnet_output = CrossNetInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ crossnet_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr crossnet"},{"location":"examples/ctr_deepfm/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) fm_output = FMInteraction ()([ sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ fm_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr deepfm"},{"location":"examples/ctr_deepfm_test_avazu/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import numpy as np import time from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor from autorecsys.utils.common import set_seed # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) avazu = AvazuPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = avazu . preprocess () train_X = avazu . get_x_categorical ( train_X ) val_X = avazu . get_x_categorical ( val_X ) sparse_input_node = Input ( shape = [ len ( avazu . categorical_columns )]) # Step 1: Mapper sparse_feat_emb = SparseFeatureMapper ( num_of_fields = len ( avazu . categorical_columns ), hash_size = avazu . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) # Step 2.1: Interactor fm_output = FMInteraction ()([ sparse_feat_emb ]) # Step 2.2: Interactor top_mlp_output = MLPInteraction ()([ fm_output ]) # Step 3: Optimizer output = PointWiseOptimizer ()( top_mlp_output ) # Step 4: Recommender wrapper model = CTRRecommender ( inputs = [ sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True },) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 1024 ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting BinaryCrossentropy: {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr deepfm test avazu"},{"location":"examples/ctr_deepfm_test_criteo/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import numpy as np import time from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor from autorecsys.utils.common import set_seed # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) set_seed ( 0 ) st = time . time () criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () print ( \"Preprocessing time: \\t \" , time . time () - st ) # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) fm_output = FMInteraction ()([ sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ fm_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 1024 ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting BinaryCrossentropy: {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y ))) print ( \"Total time: \\t \" , time . time () - st )","title":"Ctr deepfm test criteo"},{"location":"examples/ctr_dlrm/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) sparse_feat_mlp_output = MLPInteraction ()([ sparse_feat_emb ]) dense_feat_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ( num_layers = 2 )([ sparse_feat_mlp_output , dense_feat_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr dlrm"},{"location":"examples/ctr_neumf/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , PointWiseOptimizer , ElementwiseInteraction from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. input = Input ( shape = [ criteo . get_categorical_count ()]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) innerproduct_output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb_gmf , item_emb_gmf ]) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = PointWiseOptimizer ()([ innerproduct_output , mlp_output ]) model = CTRRecommender ( inputs = input , outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 10 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 256 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_categorical ( val_X )]))) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr neumf"},{"location":"examples/ctr_test_criteo/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import numpy as np import time from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , SelfAttentionInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor st = time . time () # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo_path = \"./examples/datasets/criteo_full/train.txt\" criteo = CriteoPreprocessor ( criteo_path ) criteo . preprocessing ( test_size = 0.2 ) train_X , train_y , val_X , val_y = criteo . train_X , criteo . train_y , criteo . val_X , criteo . val_y # TODO: preprocess train val split # build the pipeline. dense_input_node = Input ( shape = [ criteo . numer_num ]) sparse_input_node = Input ( shape = [ criteo . categ_num ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . numer_num , embedding_dim = 2 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . categ_num , hash_size = criteo . hash_sizes , embedding_dim = 2 )( sparse_input_node ) attention_output = SelfAttentionInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ attention_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 1000 ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y ))) print ( time . time () - st )","title":"Ctr test criteo"},{"location":"examples/netflix/","text":"import time begin = time . time () netflix = NetflixPrizePreprocessor () netflix . preprocess () print ( time . time () - begin )","title":"Netflix"},{"location":"examples/rp_autorec/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"2\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , HyperInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ##Netflix Dataset # dataset_paths = [\"./examples/datasets/netflix-prize-data/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) #Movielens 1M Dataset movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () user_num , item_num = movielens . get_hash_size () # build the pipeline. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output1 = HyperInteraction ()([ user_emb , item_emb ]) output2 = HyperInteraction ()([ output1 , user_emb , item_emb ]) output3 = HyperInteraction ()([ output1 , output2 , user_emb , item_emb ]) output4 = HyperInteraction ()([ output1 , output2 , output3 , user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output4 ) model = RPRecommender ( inputs = input , outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , ## hyperband, bayesian tuner_params = { 'max_trials' : 2 , 'overwrite' : True },) searcher . search ( x = [ movielens . get_x_categorical ( train_X )], y = train_y , x_val = [ movielens . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 1 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Predicting Val Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( val_X ), y_true = val_y ))) logger . info ( 'Predicting Test Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( test_X ), y_true = test_y )))","title":"Rp autorec"},{"location":"examples/rp_benchmark/","text":"import argparse import time import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"5\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , HyperInteraction , MLPInteraction , \\ ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor , NetflixPrizePreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) def build_mf ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model def build_gmf ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model def build_mlp ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = user_num , embedding_dim = 64 )( input ) output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model def build_neumf ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) innerproduct_output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb_gmf , item_emb_gmf ]) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = RatingPredictionOptimizer ()([ innerproduct_output , mlp_output ]) model = RPRecommender ( inputs = input , outputs = output ) return model def build_autorec ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb_1 = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_1 = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) user_emb_2 = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_2 = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = HyperInteraction ()([ user_emb_1 , item_emb_1 , user_emb_2 , item_emb_2 ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model if __name__ == '__main__' : # parse args parser = argparse . ArgumentParser () parser . add_argument ( '-model' , type = str , help = 'input a model name' ) parser . add_argument ( '-data' , type = str , help = 'dataset name' ) parser . add_argument ( '-data_path' , type = str , help = 'dataset path' ) parser . add_argument ( '-sep' , type = str , help = 'dataset sep' ) parser . add_argument ( '-search' , type = str , help = 'input a search method name' ) parser . add_argument ( '-batch_size' , type = int , help = 'batch size' ) parser . add_argument ( '-epochs' , type = int , help = 'epochs' ) parser . add_argument ( '-early_stop' , type = int , help = 'early stop' ) parser . add_argument ( '-trials' , type = int , help = 'try number' ) args = parser . parse_args () # print(\"args:\", args) if args . sep == None : args . sep = '::' # Load dataset if args . data == \"ml\" : data = MovielensPreprocessor ( csv_path = args . data_path , validate_percentage = 0.1 , test_percentage = 0.1 ) train_X , train_y , val_X , val_y , test_X , test_y = data . preprocess () user_num , item_num = data . get_hash_size () # if args.data == \"netflix\": # dataset_paths = [args.data_path + \"/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) logger . info ( 'train_X size: {} ' . format ( train_X . shape )) logger . info ( 'train_y size: {} ' . format ( train_y . shape )) logger . info ( 'val_X size: {} ' . format ( val_X . shape )) logger . info ( 'val_y size: {} ' . format ( val_y . shape )) logger . info ( 'test_X size: {} ' . format ( test_X . shape )) logger . info ( 'test_y size: {} ' . format ( test_y . shape )) logger . info ( 'user total number: {} ' . format ( user_num )) logger . info ( 'item total number: {} ' . format ( item_num )) # select model if args . model == 'mf' : model = build_mf ( user_num , item_num ) if args . model == 'mlp' : model = build_mlp ( user_num , item_num ) if args . model == 'gmf' : model = build_gmf ( user_num , item_num ) if args . model == 'neumf' : model = build_neumf ( user_num , item_num ) if args . model == 'autorec' : model = build_autorec ( user_num , item_num ) # search and predict. searcher = Search ( model = model , tuner = args . search , tuner_params = { 'max_trials' : args . trials , 'overwrite' : True } ) start_time = time . time () searcher . search ( x = data . get_x_categorical ( train_X ), y = train_y , x_val = data . get_x_categorical ( val_X ), y_val = val_y , objective = 'val_mse' , batch_size = args . batch_size , epochs = args . epochs , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = args . early_stop )]) end_time = time . time () # print(\"Runing time:\", end_time - start_time) # print(\"Args\", args) logger . info ( 'Runing time: {} ' . format ( end_time - start_time )) logger . info ( 'Args: {} ' . format ( args )) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = data . get_x_categorical ( test_X ), y_true = test_y )))","title":"Rp benchmark"},{"location":"examples/rp_mf/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"5\" import tensorflow as tf # gpus = tf.config.experimental.list_physical_devices(device_type='GPU') # for gpu in gpus: # tf.config.experimental.set_memory_growth(gpu, True) # import tensorflow as tf # physical_devices = tf.config.list_physical_devices('GPU') # tf.config.experimental.set_memory_growth(physical_devices[0], True) import logging from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor , NetflixPrizePreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ##Netflix Dataset # dataset_paths = [\"./examples/datasets/netflix-prize-data/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) #Movielens 1M Dataset movielens = NetflixPrizePreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () user_num , item_num = movielens . get_hash_size () # build the pipeline. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) # AutoML search and predict searcher = Search ( model = model , tuner = 'greedy' , # hyperband, greedy, bayesian tuner_params = { \"max_trials\" : 5 } ) searcher . search ( x = [ movielens . get_x_categorical ( train_X )], y = train_y , x_val = [ movielens . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Predicting Val Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( val_X ), y_true = val_y ))) logger . info ( 'Predicting Test Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( test_X ), y_true = test_y )))","title":"Rp mf"},{"location":"examples/rp_neumf/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , RatingPredictionOptimizer , \\ ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ##Netflix Dataset # dataset_paths = [\"./examples/datasets/netflix-prize-data/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) #Movielens 1M Dataset movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () user_num , item_num = movielens . get_hash_size () # build the pipeline. input = Input ( shape = [ 2 ]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) innerproduct_output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb_gmf , item_emb_gmf ]) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = RatingPredictionOptimizer ()([ innerproduct_output , mlp_output ]) model = RPRecommender ( inputs = input , outputs = output ) # AutoML search and predict searcher = Search ( model = model , tuner = 'greedy' , # random, greedy tuner_params = { \"max_trials\" : 5 , 'overwrite' : True } ) searcher . search ( x = [ movielens . get_x_categorical ( train_X )], y = train_y , x_val = [ movielens . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 1 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Predicting Val Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( val_X ), y_true = val_y ))) logger . info ( 'Predicting Test Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( test_X ), y_true = test_y )))","title":"Rp neumf"},{"location":"tutorials/","text":"AutoRec Tutorials Install jupyter notebooks and run jupyter notebook in /docs/ipynb/ Rewrite python scripts in /examples/ into a notebook format Export ipynb as md in jupyter Add colab and source button code to the top of the generated .md file :octicons-link-16: [**View in Colab**](https://colab.research.google.com/github/datamllab/AutoRec/blob/master/docs/ipynb/name-of-the-notebook.ipynb) &nbsp; :fontawesome-brands-github-alt: [**GitHub source**](https://github.com/datamllab/AutoRec/tree/master/examples/name-of-the-script.py)","title":"Index"},{"location":"tutorials/#autorec-tutorials","text":"Install jupyter notebooks and run jupyter notebook in /docs/ipynb/ Rewrite python scripts in /examples/ into a notebook format Export ipynb as md in jupyter Add colab and source button code to the top of the generated .md file :octicons-link-16: [**View in Colab**](https://colab.research.google.com/github/datamllab/AutoRec/blob/master/docs/ipynb/name-of-the-notebook.ipynb) &nbsp; :fontawesome-brands-github-alt: [**GitHub source**](https://github.com/datamllab/AutoRec/tree/master/examples/name-of-the-script.py)","title":"AutoRec Tutorials"},{"location":"tutorials/CTR_AutoInt/","text":"View in Colab GitHub source ! pip install autorec AutoInt Click-through rate (CTR) https://arxiv.org/pdf/1810.11921.pdf Description: AutoInt learns high-order feature interactions effectively and efficiently by applying both numerical and categorical input features into the model. AutoInt's multi-head self-attentive neural network learns low-order feature interactions. A simple example below Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , DenseFeatureMapper , SparseFeatureMapper , SelfAttentionInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has two interactors. Any of other interactors can be stacked of your choice. attention_output = SelfAttentionInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ attention_output , bottom_mlp_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate the accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"CTR AutoInt"},{"location":"tutorials/CTR_AutoInt/#autoint","text":"Click-through rate (CTR) https://arxiv.org/pdf/1810.11921.pdf Description: AutoInt learns high-order feature interactions effectively and efficiently by applying both numerical and categorical input features into the model. AutoInt's multi-head self-attentive neural network learns low-order feature interactions.","title":"AutoInt"},{"location":"tutorials/CTR_AutoInt/#a-simple-example-below","text":"Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , DenseFeatureMapper , SparseFeatureMapper , SelfAttentionInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has two interactors. Any of other interactors can be stacked of your choice. attention_output = SelfAttentionInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ attention_output , bottom_mlp_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate the accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"A simple example below"},{"location":"tutorials/CTR_AutoRec/","text":"View in Colab GitHub source ! pip install autorec AutoRec Click-through rate Description: Our AutoRec has a specaility when you want to search a model without any prior knowledge. Autorec selects different blocks in the interactor. Autorec is especially useful for 1) users who need the optimal model after systematic exploration and 2) users who wants to contemplate about the intuition behind the searched model. A simple example below Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , HyperInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has one interactor that is used three times. Any of other interactors can be stacked of your choice. sparse_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_emb ]) dense_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ dense_feat_emb ]) hyper_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_bottom_output , dense_feat_bottom_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( hyper_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"CTR AutoRec"},{"location":"tutorials/CTR_AutoRec/#autorec","text":"Click-through rate Description: Our AutoRec has a specaility when you want to search a model without any prior knowledge. Autorec selects different blocks in the interactor. Autorec is especially useful for 1) users who need the optimal model after systematic exploration and 2) users who wants to contemplate about the intuition behind the searched model.","title":"AutoRec"},{"location":"tutorials/CTR_AutoRec/#a-simple-example-below","text":"Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , HyperInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has one interactor that is used three times. Any of other interactors can be stacked of your choice. sparse_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_emb ]) dense_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ dense_feat_emb ]) hyper_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_bottom_output , dense_feat_bottom_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( hyper_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"A simple example below"},{"location":"tutorials/CTR_CrossNet/","text":"View in Colab GitHub source ! pip install autorec CrossNet Click-through rate (CTR) https://arxiv.org/pdf/1708.05123.pdf Description: Deep Cross Network (DCN) is a combination of DNN models that are good for complicated features and cross networks that are good for bounded degree feature interactions. Cross network produces feature interactions at each layer, and keeps the interactions from the previous layer. This algorithm helps to efficiently learn bounded features and restrains from expensive searches. Both large datasets of sparse and dense layers can be advantaged in both accuracy and memory usage by this model. A simple example below Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , CrossNetInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. (link) Input, DenseFeatureMapper, SparseFeatureMapper dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has two interactors. Any of other interactors can be stacked of your choice. crossnet_output = CrossNetInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ crossnet_output , bottom_mlp_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher Search the best model and validate the accuracy of the model This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"CTR CrossNet"},{"location":"tutorials/CTR_CrossNet/#crossnet","text":"Click-through rate (CTR) https://arxiv.org/pdf/1708.05123.pdf Description: Deep Cross Network (DCN) is a combination of DNN models that are good for complicated features and cross networks that are good for bounded degree feature interactions. Cross network produces feature interactions at each layer, and keeps the interactions from the previous layer. This algorithm helps to efficiently learn bounded features and restrains from expensive searches. Both large datasets of sparse and dense layers can be advantaged in both accuracy and memory usage by this model.","title":"CrossNet"},{"location":"tutorials/CTR_CrossNet/#a-simple-example-below","text":"Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , CrossNetInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. (link) Input, DenseFeatureMapper, SparseFeatureMapper dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has two interactors. Any of other interactors can be stacked of your choice. crossnet_output = CrossNetInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ crossnet_output , bottom_mlp_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher Search the best model and validate the accuracy of the model This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"A simple example below"},{"location":"tutorials/CTR_DLRM/","text":"View in Colab GitHub source ! pip install autorec DLRM Click-through rate (CTR) https://arxiv.org/pdf/1906.00091.pdf Description: A simple example below Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 ] dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has one stacked interactor. Any of other interactors can be stacked of your choice. sparse_feat_mlp_output = MLPInteraction ()([ sparse_feat_emb ]) dense_feat_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ( num_layers = 2 )([ sparse_feat_mlp_output , dense_feat_mlp_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"CTR DLRM"},{"location":"tutorials/CTR_DLRM/#dlrm","text":"Click-through rate (CTR) https://arxiv.org/pdf/1906.00091.pdf Description:","title":"DLRM"},{"location":"tutorials/CTR_DLRM/#a-simple-example-below","text":"Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 ] dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has one stacked interactor. Any of other interactors can be stacked of your choice. sparse_feat_mlp_output = MLPInteraction ()([ sparse_feat_emb ]) dense_feat_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ( num_layers = 2 )([ sparse_feat_mlp_output , dense_feat_mlp_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"A simple example below"},{"location":"tutorials/CTR_DeepFM/","text":"View in Colab GitHub source ! pip install autorec DeepFM Click-through rate (CTR) https://arxiv.org/pdf/1703.04247.pdf Description: DeepFM captures both low and high order feature interactions without the need to learn sophisticated feature interactions in Click Through Rate (CTR) recommender system. It integrates Factorization Machines (FM) that are a pair-wise feature interaction for recommendation and Deep Neural Networks (DNN) that are powerful in learning complicated feature interactions. DeepFM advantages in 1) no need for pretraining 2) learns both low and high feature interactions 3) shares the same inputs/embedding for no feature engineering. A simple example below Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has only three interactors. Any of other interactors can be stacked of your choice. fm_output = FMInteraction ()([ sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ fm_output , bottom_mlp_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"CTR DeepFM"},{"location":"tutorials/CTR_DeepFM/#deepfm","text":"Click-through rate (CTR) https://arxiv.org/pdf/1703.04247.pdf Description: DeepFM captures both low and high order feature interactions without the need to learn sophisticated feature interactions in Click Through Rate (CTR) recommender system. It integrates Factorization Machines (FM) that are a pair-wise feature interaction for recommendation and Deep Neural Networks (DNN) that are powerful in learning complicated feature interactions. DeepFM advantages in 1) no need for pretraining 2) learns both low and high feature interactions 3) shares the same inputs/embedding for no feature engineering.","title":"DeepFM"},{"location":"tutorials/CTR_DeepFM/#a-simple-example-below","text":"Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. dense_input_node = Input ( shape = [ numerical_count ]) # shape=13 sparse_input_node = Input ( shape = [ categorical_count ]) # shape=26 dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) Step 2.2: Setup interactors to handle models This example has only three interactors. Any of other interactors can be stacked of your choice. fm_output = FMInteraction ()([ sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ fm_output , bottom_mlp_output ]) Step 2.3: Setup optimizer to handle the target task output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Validation Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ val_X_numerical , val_X_categorical ], y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = test_y )))","title":"A simple example below"},{"location":"tutorials/CTR_NeuMF/","text":"View in Colab GitHub source ! pip install autorec Neural Matrix Factorization (NeuMF) Click-through rate https://arxiv.org/pdf/1708.05031.pdf Description: NeuMF captures the personalized ranking task with implicit feedback with the use of both linearity and non-linearity. NeuMF consists of two subnetwork GMF and DNN layers. GMF layer computes the elementwise product of user and item latent factors. DNN layers concatenates user and item factors then uses multi-layer perceptron (MLP). Combining linearity and non-linearity of models, NeuMF leverages advantages of both subnetwork: generality and flexibility. A simple example below Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , PointWiseOptimizer from autorecsys.pipeline.interactor import InnerProductInteraction from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. This examples requires to embed into two parts: gmf & mlp. input = Input ( shape = [ criteo . get_categorical_count ()]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) Step 2.2: Setup interactors to handle models This example has two interactors. Any of other interactors can be stacked of your choice. innerproduct_output = InnerProductInteraction ()([ user_emb_gmf , item_emb_gmf ]) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) Step 2.3: Setup optimizer to handle the target task Two interactors (innerproduct and mlp) are concatenated and optimized. output = PointWiseOptimizer ()([ innerproduct_output , mlp_output ]) model = CTRRecommender ( inputs = input , outputs = output ) Step 3: Build the searcher This provides the search algorithm searcher = Search ( model = model , tuner = 'random' , # hyperband, greedy, bayesian tuner_params = { \"max_trials\" : 5 } ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 256 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) Step 5: Evaluate the searched model logger . info ( 'Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_categorical ( val_X )]))) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"CTR NeuMF"},{"location":"tutorials/CTR_NeuMF/#neural-matrix-factorization-neumf","text":"Click-through rate https://arxiv.org/pdf/1708.05031.pdf Description: NeuMF captures the personalized ranking task with implicit feedback with the use of both linearity and non-linearity. NeuMF consists of two subnetwork GMF and DNN layers. GMF layer computes the elementwise product of user and item latent factors. DNN layers concatenates user and item factors then uses multi-layer perceptron (MLP). Combining linearity and non-linearity of models, NeuMF leverages advantages of both subnetwork: generality and flexibility.","title":"Neural Matrix Factorization (NeuMF)"},{"location":"tutorials/CTR_NeuMF/#a-simple-example-below","text":"Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , PointWiseOptimizer from autorecsys.pipeline.interactor import InnerProductInteraction from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data CriteoPreprocessor() is a already built-in preprocessor for the Criteo dataset. # Step 1: Preprocess data criteo = CriteoPreprocessor () # the default arguments are setup to preprocess the Criteo example dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. This examples requires to embed into two parts: gmf & mlp. input = Input ( shape = [ criteo . get_categorical_count ()]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) Step 2.2: Setup interactors to handle models This example has two interactors. Any of other interactors can be stacked of your choice. innerproduct_output = InnerProductInteraction ()([ user_emb_gmf , item_emb_gmf ]) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) Step 2.3: Setup optimizer to handle the target task Two interactors (innerproduct and mlp) are concatenated and optimized. output = PointWiseOptimizer ()([ innerproduct_output , mlp_output ]) model = CTRRecommender ( inputs = input , outputs = output ) Step 3: Build the searcher This provides the search algorithm searcher = Search ( model = model , tuner = 'random' , # hyperband, greedy, bayesian tuner_params = { \"max_trials\" : 5 } ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 256 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) Step 5: Evaluate the searched model logger . info ( 'Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_categorical ( val_X )]))) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"A simple example below"},{"location":"tutorials/Overview/","text":"AutoRec 1.0 Tutorial Supported Task AutoRec supports two tasks with several built-in models. You can click the link below to see the details of each task/models. Click-Through rate (CTR) CTR AutoInt CTR AutoRec CTR CrossNet CTR DeepFM CTR DLRM CTR NeuMF Rating Prediction (RP) RP MF RP NeuMF Searchable Customized Model You can quickly construct a high level architecture for your own searchable model. AutoRec will search for the best model. CTR AutoRec RP AutoRec Customizable Nodes/Blocks These are predefined input Nodes/Blocks where you can customize your own stacked up model. You can click the link below to redirect. Preprocessor BasePreprocessor Node Input Blocks Mapper DenseFeatureMapper SparseFeatureMapper Interactor RandomSelectInteraction ConcatenateInteraction InnerProductInteraction ElementwiseInteraction MLPInteraction HyperInteraction FMInteraction CrossNetInteraction SelfAttentionInteraction Optimizer CTROptimizer RPOptimizer","title":"Overview"},{"location":"tutorials/Overview/#autorec-10-tutorial","text":"","title":"AutoRec 1.0 Tutorial"},{"location":"tutorials/Overview/#supported-task","text":"AutoRec supports two tasks with several built-in models. You can click the link below to see the details of each task/models. Click-Through rate (CTR) CTR AutoInt CTR AutoRec CTR CrossNet CTR DeepFM CTR DLRM CTR NeuMF Rating Prediction (RP) RP MF RP NeuMF","title":"Supported Task"},{"location":"tutorials/Overview/#searchable-customized-model","text":"You can quickly construct a high level architecture for your own searchable model. AutoRec will search for the best model. CTR AutoRec RP AutoRec","title":"Searchable Customized Model"},{"location":"tutorials/Overview/#customizable-nodesblocks","text":"These are predefined input Nodes/Blocks where you can customize your own stacked up model. You can click the link below to redirect.","title":"Customizable Nodes/Blocks"},{"location":"tutorials/Overview/#preprocessor","text":"BasePreprocessor","title":"Preprocessor"},{"location":"tutorials/Overview/#node","text":"Input","title":"Node"},{"location":"tutorials/Overview/#blocks","text":"Mapper DenseFeatureMapper SparseFeatureMapper","title":"Blocks"},{"location":"tutorials/Overview/#interactor","text":"RandomSelectInteraction ConcatenateInteraction InnerProductInteraction ElementwiseInteraction MLPInteraction HyperInteraction FMInteraction CrossNetInteraction SelfAttentionInteraction","title":"Interactor"},{"location":"tutorials/Overview/#optimizer","text":"CTROptimizer RPOptimizer","title":"Optimizer"},{"location":"tutorials/RP_AutoRec/","text":"View in Colab GitHub source ! pip install autorec AutoRec Rating Prediction Description: Our AutoRec has a specaility when you want to search a model without any prior knowledge. Autorec selects different blocks in the interactor. Autorec is especially useful for 1) users who need the optimal model after systematic exploration and 2) users who wants to contemplate about the intuition behind the searched model. A simple example below Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"2\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , HyperInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data MovielensPreprocessor() is a already built-in preprocessor for the Movielens 1M dataset. Load the dataset, which in this case is set to the Movielens 1 million dataset, although options exist for a 10 million, latest, and Netflix dataset. # Step 1: Preprocess data movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () train_X_categorical = movielens . get_x_categorical ( train_X ) val_X_categorical = movielens . get_x_categorical ( val_X ) test_X_categorical = movielens . get_x_categorical ( test_X ) user_num , item_num = movielens . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) Step 2.2: Setup interactors to handle models This example has one interactor that is used three times. Any of other interactors can be stacked of your choice. output1 = HyperInteraction ()([ user_emb , item_emb ]) output2 = HyperInteraction ()([ output1 , user_emb , item_emb ]) output3 = HyperInteraction ()([ output1 , output2 , user_emb , item_emb ]) output4 = HyperInteraction ()([ output1 , output2 , output3 , user_emb , item_emb ]) Step 2.3: Setup optimizer to handle the target task output = RatingPredictionOptimizer ()( output4 ) model = RPRecommender ( inputs = input , outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True },) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_categorical ], y = train_y , x_val = [ val_X_categorical ], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Validation Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X_categorical , y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X_categorical , y_true = test_y )))","title":"RP AutoRec"},{"location":"tutorials/RP_AutoRec/#autorec","text":"Rating Prediction Description: Our AutoRec has a specaility when you want to search a model without any prior knowledge. Autorec selects different blocks in the interactor. Autorec is especially useful for 1) users who need the optimal model after systematic exploration and 2) users who wants to contemplate about the intuition behind the searched model.","title":"AutoRec"},{"location":"tutorials/RP_AutoRec/#a-simple-example-below","text":"Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"2\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , HyperInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data MovielensPreprocessor() is a already built-in preprocessor for the Movielens 1M dataset. Load the dataset, which in this case is set to the Movielens 1 million dataset, although options exist for a 10 million, latest, and Netflix dataset. # Step 1: Preprocess data movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () train_X_categorical = movielens . get_x_categorical ( train_X ) val_X_categorical = movielens . get_x_categorical ( val_X ) test_X_categorical = movielens . get_x_categorical ( test_X ) user_num , item_num = movielens . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Criteo dataset has both dense & sparse inputs. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) Step 2.2: Setup interactors to handle models This example has one interactor that is used three times. Any of other interactors can be stacked of your choice. output1 = HyperInteraction ()([ user_emb , item_emb ]) output2 = HyperInteraction ()([ output1 , user_emb , item_emb ]) output3 = HyperInteraction ()([ output1 , output2 , user_emb , item_emb ]) output4 = HyperInteraction ()([ output1 , output2 , output3 , user_emb , item_emb ]) Step 2.3: Setup optimizer to handle the target task output = RatingPredictionOptimizer ()( output4 ) model = RPRecommender ( inputs = input , outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True },) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_categorical ], y = train_y , x_val = [ val_X_categorical ], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Validation Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X_categorical , y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X_categorical , y_true = test_y )))","title":"A simple example below"},{"location":"tutorials/RP_MF/","text":"View in Colab GitHub source ! pip install autorec Matrix Factorization (MF) Rating Prediction https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf Description: Factorization Machine is a supervised learning algorithm that obtains feature interactions under both density and sparsity. The model captures all single and pairwise interactions by factorizing each and every parameters; the model is calculated in linear time and depends on only linear parameters. Unlike other models which are specialized for specific tasks, FM is built for general purpose that can be handled with any real valued vector inputs. A simple example below Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"5\" import tensorflow as tf import logging from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer from autorecsys.pipeline.interactor import InnerProductInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data MovielensPreprocessor() is a already built-in preprocessor for the Movielens 1M dataset. Load the dataset, which in this case is set to the Movielens 1 million dataset, although options exist for a 10 million, latest, and Netflix dataset. # Step 1: Preprocess data movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () train_X_categorical = movielens . get_x_categorical ( train_X ) val_X_categorical = movielens . get_x_categorical ( val_X ) test_X_categorical = movielens . get_x_categorical ( test_X ) user_num , item_num = movielens . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Then embed into a dimension of your choice. This example, embedding dim is declared to 64. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) Step 2.2: Setup interactors to handle models This example has only one interactor. Any of other interactors can be stacked of your choice. output = InnerProductInteraction ()([ user_emb , item_emb ]) Step 2.3: Setup optimizer to handle the target task output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'greedy' , # random, greedy tuner_params = { \"max_trials\" : 5 , 'overwrite' : True } Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_categorical ], y = train_y , x_val = [ val_X_categorical ], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Validation Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X_categorical , y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X_categorical , y_true = test_y )))","title":"RP MF"},{"location":"tutorials/RP_MF/#matrix-factorization-mf","text":"Rating Prediction https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf Description: Factorization Machine is a supervised learning algorithm that obtains feature interactions under both density and sparsity. The model captures all single and pairwise interactions by factorizing each and every parameters; the model is calculated in linear time and depends on only linear parameters. Unlike other models which are specialized for specific tasks, FM is built for general purpose that can be handled with any real valued vector inputs.","title":"Matrix Factorization (MF)"},{"location":"tutorials/RP_MF/#a-simple-example-below","text":"Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"5\" import tensorflow as tf import logging from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer from autorecsys.pipeline.interactor import InnerProductInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data MovielensPreprocessor() is a already built-in preprocessor for the Movielens 1M dataset. Load the dataset, which in this case is set to the Movielens 1 million dataset, although options exist for a 10 million, latest, and Netflix dataset. # Step 1: Preprocess data movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () train_X_categorical = movielens . get_x_categorical ( train_X ) val_X_categorical = movielens . get_x_categorical ( val_X ) test_X_categorical = movielens . get_x_categorical ( test_X ) user_num , item_num = movielens . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. Then embed into a dimension of your choice. This example, embedding dim is declared to 64. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) Step 2.2: Setup interactors to handle models This example has only one interactor. Any of other interactors can be stacked of your choice. output = InnerProductInteraction ()([ user_emb , item_emb ]) Step 2.3: Setup optimizer to handle the target task output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) Step 3: Build the searcher This provides the search algorithm. searcher = Search ( model = model , tuner = 'greedy' , # random, greedy tuner_params = { \"max_trials\" : 5 , 'overwrite' : True } Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_categorical ], y = train_y , x_val = [ val_X_categorical ], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Validation Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X_categorical , y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X_categorical , y_true = test_y )))","title":"A simple example below"},{"location":"tutorials/RP_NeuMF/","text":"View in Colab GitHub source ! pip install autorec Neural Matrix Factorization (NeuMF) Rating Prediction https://arxiv.org/pdf/1708.05031.pdf Description: NeuMF captures the personalized ranking task with implicit feedback with the use of both linearity and non-linearity. NeuMF consists of two subnetwork GMF and DNN layers. GMF layer computes the elementwise product of user and item latent factors. DNN layers concatenates user and item factors then uses multi-layer perceptron (MLP). Combining linearity and non-linearity of models, NeuMF leverages advantages of both subnetwork: generality and flexibility. A simple example below Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , RatingPredictionOptimizer from autorecsys.pipeline.interactor import InnerProductInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data MovielensPreprocessor() is already built-in preprocessor for the Movielens 1M dataset. Load the dataset, which in this case is set to the Movielens 1 million dataset, although options exist for a 10 million, latest, and Netflix dataset. # Step 1: Preprocess data movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () train_X_categorical = movielens . get_x_categorical ( train_X ) val_X_categorical = movielens . get_x_categorical ( val_X ) test_X_categorical = movielens . get_x_categorical ( test_X ) user_num , item_num = movielens . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. This examples requires to embed into two parts: gmf & mlp. input = Input ( shape = [ 2 ]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) Step 2.2: Setup interactors to handle models This example has two interactors. Any of other interactors can be stacked of your choice. innerproduct_output = InnerProductInteraction ()([ user_emb_gmf , item_emb_gmf ]) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) Step 2.3: Setup optimizer to handle the target task Two interactors (innerproduct and mlp) are concatenated and optimized. innerproduct_output = InnerProductInteraction ()([ user_emb_gmf , item_emb_gmf ]) model = RPRecommender ( inputs = input , outputs = output ) Step 3: Build the searcher This provides the search algorithm searcher = Search ( model = model , tuner = 'greedy' , # hyperband, greedy, bayesian tuner_params = { \"max_trials\" : 5 } ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_categorical ], y = train_y , x_val = [ val_X_categorical ], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Validation Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X_categorical , y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X_categorical , y_true = test_y )))","title":"RP NeuMF"},{"location":"tutorials/RP_NeuMF/#neural-matrix-factorization-neumf","text":"Rating Prediction https://arxiv.org/pdf/1708.05031.pdf Description: NeuMF captures the personalized ranking task with implicit feedback with the use of both linearity and non-linearity. NeuMF consists of two subnetwork GMF and DNN layers. GMF layer computes the elementwise product of user and item latent factors. DNN layers concatenates user and item factors then uses multi-layer perceptron (MLP). Combining linearity and non-linearity of models, NeuMF leverages advantages of both subnetwork: generality and flexibility.","title":"Neural Matrix Factorization (NeuMF)"},{"location":"tutorials/RP_NeuMF/#a-simple-example-below","text":"Step 0:Imports and Configurations First, handle the imports with the correct configurations set. Also include the logging settings here. from __future__ import absolute_import , division , print_function , unicode_literals import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , RatingPredictionOptimizer from autorecsys.pipeline.interactor import InnerProductInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) Step 1: Preprocess data MovielensPreprocessor() is already built-in preprocessor for the Movielens 1M dataset. Load the dataset, which in this case is set to the Movielens 1 million dataset, although options exist for a 10 million, latest, and Netflix dataset. # Step 1: Preprocess data movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () train_X_categorical = movielens . get_x_categorical ( train_X ) val_X_categorical = movielens . get_x_categorical ( val_X ) test_X_categorical = movielens . get_x_categorical ( test_X ) user_num , item_num = movielens . get_hash_size () Step 2.0: Build the recommender Creates the pipeline Models can be customized for a searchable recommender Step 2.1: Setup mappers to handle inputs This step is to build a input node for your data. This examples requires to embed into two parts: gmf & mlp. input = Input ( shape = [ 2 ]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) Step 2.2: Setup interactors to handle models This example has two interactors. Any of other interactors can be stacked of your choice. innerproduct_output = InnerProductInteraction ()([ user_emb_gmf , item_emb_gmf ]) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) Step 2.3: Setup optimizer to handle the target task Two interactors (innerproduct and mlp) are concatenated and optimized. innerproduct_output = InnerProductInteraction ()([ user_emb_gmf , item_emb_gmf ]) model = RPRecommender ( inputs = input , outputs = output ) Step 3: Build the searcher This provides the search algorithm searcher = Search ( model = model , tuner = 'greedy' , # hyperband, greedy, bayesian tuner_params = { \"max_trials\" : 5 } ) Step 4: Use the searcher to search the recommender Search the best model and validate accuracy of the model Inputs such as objective, batch_size, call backs can be customized for you own model. searcher . search ( x = [ train_X_categorical ], y = train_y , x_val = [ val_X_categorical ], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Validation Accuracy (mse): {} ' . format ( searcher . evaluate ( x = val_X_categorical , y_true = val_y ))) Step 5: Evaluate the searched model logger . info ( 'Test Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X_categorical , y_true = test_y )))","title":"A simple example below"}]}